%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% TB VSA Pipeline documentation
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,10pt,twoside]{article}

\usepackage{float}
\usepackage[justification=raggedright,singlelinecheck=false,skip=5pt,font=small]{caption}
\usepackage[sc]{mathpazo}
\usepackage{microtype}
\usepackage{sectsty}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{fancyhdr} 
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyvrb}
\usepackage{hyperref}

\linespread{1.05}
\setlength{\headsep}{20pt}
\setlength{\headheight}{12pt}

\usepackage[compact]{titlesec}

\usepackage{geometry}
\geometry{ a4paper, left=20mm, top=20mm, right=20mm}

\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % clear default header
\fancyfoot{}  % clear default footer
\fancyhead[L]{TB Variable Site Alignment Pipeline} % left justified header content
\fancyhead[R]{Version 0.1.2} % right justified header content
\fancyfoot[LE,RO]{\thepage} %alternate page numbering
\renewcommand{\familydefault}{\sfdefault} % set default font family
\allsectionsfont{\sffamily\raggedright}
\captionsetup{
  font=footnotesize,
  justification=raggedright,
  singlelinecheck=false
}

\newenvironment{tight_enumerate}{
\begin{enumerate}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
}{\end{enumerate}}

\newenvironment{tight_itemize}{
\begin{itemize}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
}{\end{itemize}}

\title{TB Variable Site Alignment Pipeline}
\date{\today}
\author{James Abbott (j.abbott\@imperial.ac.uk)}

\begin{document}
\maketitle
\thispagestyle{fancy} % All pages have headers and footers

\tableofcontents

\section{Introduction}

This document describes a pipeline for generating variable-site alignments
(VSA) of populations of Mycobacterium tuberculosis isolates from paired-end
Illumina WGS data. This has been produced on behalf of Caroline Colijn, Dept of
Mathematics, Imperial College London. The pipeline carries out data QC and
read-trimming/adapter removal, followed by alignment of the sequence reads to a
reference TB strain using BWA.  Duplicate reads are marked, and local
realignment of reads around potential indel sites is then carried out. SNVs are
then identified using FreeBayes. An alignment consisting of only the variable
sites in the population is then created excluding hyper-variable regions.

The pipeline has been designed to run on a cluster (cx1) under a batch queuing
system (PBS Pro). Moving the software to run under a different environment will
require modification of PBS directives etc. accordingly.

Note that this pipeline has been produced specifically for TB isolates,
considering the low recombination rates present in this organism, consequently
is not necessarily generally applicable to other species.

\subsection{Prerequisites}

\subsubsection{Software packages}

A number of software packages are necessary to run this pipeline. These should
be installed so that their binaries are available on the default path in the
environment the software will be executed under. If run on the cx1 cluster, the
appropriate environment modules will be automatically loaded at runtime so no
action is required to add the binaries to the path. The required packages are
listed in table \ref{tab:01}.Other versions of these packages may well work,
however the results of using an untested version can not be predicted.

\begin{table}
{\scriptsize
\begin{tabularx}{400pt}{@{}lll@{}}\toprule
Package	& Tested Version & Availability \\\midrule
bwa		    &	0.7.15 & \url{http://bio-bwa.sourceforge.net} \\
cutadapt	&	1.10  & \url{https://github.com/marcelm/cutadapt} \\
fastqc		&	0.11.2 & \url{https://www.bioinformatics.babraham.ac.uk/projects/fastqc} \\
freebayes	&	1.1.0 & \url{https://github.com/ekg/freebayes} \\
gatk		&	3.6 & \url{https://software.broadinstitute.org/gatk} \\
picard		&	2.6.0 & \url{https://broadinstitute.github.io/picard} \\
sambamba	&	0.6.5 & \url{http://lomereiter.github.io/sambamba}\\
samtools	&	1.3.1 & \url{http://www.htslib.org} \\
trim\_galore	&	0.4.1 & \url{https://www.bioinformatics.babraham.ac.uk/projects/trim\_galore/} \\
vt	&	0.5.77 & \url{http://genome.sph.umich.edu/wiki/Vt} \\\hline
\end{tabularx}}{}
\caption{Prerequisite software packages used by pipeline.\label{tab:01}} 
\end{table}

\subsubsection{Installing the pipeline}

The pipeline is available in the github repository located at
\url{https://github.com/ImperialCollegeLondon/TB-VSA-Pipeline}. The latest
release can be obtained as a gzip compressed tar file from
\url{https://github.com/ImperialCollegeLondon/TB-VSA-Pipeline/releases}. Once downloaded, the contents of this can be extracted using the command:

\begin{verbatim}
tar zxvf downloaded_file.tar.gz
\end{verbatim}

Alternately, the latest updates to the software can be obtained directly from the git repository by running:

\begin{verbatim}
git clone https://github.com/ImperialCollegeLondon/TB-VSA-Pipeline
\end{verbatim}

This will result in the creation of a new directory named 'TB-VSA-Pipeline'. 

\subsubsection{Perl modules}

 Certain perl modules are used by the pipeline, as indicated in table \ref{tab:02}. 
 Those which are not generally installed along with perl will be downloaded and
installed into the software installation directory when the {\tt setup.sh} script is
executed. Should you wish to use other versions of these modules already
installed on the system, it will be necessary to update the '{\tt use lib}'
statements in the scripts to include the path to the libraries you wish to use. 

\begin{table}
\scriptsize
\begin{tabularx}{250pt}{@{}lcc@{}}\toprule
Module & Core? & Notes \\\midrule
Archive::Zip & No & Only required for download scripts \\
BioPerl & No & \\
Cwd & Yes & \\
Digest::MD5::File & No & Only required for download scripts  \\
File::Basename & Yes & \\
File::Find::Rule & No & \\\
File::Path & Yes & \\
FindBin & Yes & \\
Getopt::Long & Yes & \\
IPC::Cmd & Yes & \\
IPC::Open3 & Yes & \\
LWP::UserAgent & No & Only required for download scripts  \\
Pod::Usage & Yes & \\
XML::XPath & No & Only required for download scripts \\
XML::XPath::Parser & No & Only required for download scripts \\\hline
\end{tabularx}{}
\caption{Required Perl modules.\label{tab:02}} 
\end{table}

\subsubsection {Configuring the software}

Once the software is downloaded, it is necessary to run the {\tt setup.sh}
script contained in the software installation directory. This script modifies
the script templates in the {\tt src} directory to set:

\begin{tight_enumerate}
\item the path to the preferred version of perl. 

\item the number of parallel threads to run. This defaults to the number of available cores on the system where installation is taking place.

\item the name of a PBSPro queue to submit jobs to (optional). The software can
be run manually without using PBSPro, but will require each sample analysis to
be run independently.

\item {\tt module load} commands to make the necessary software prerequisites
available on the default system path (only if installed on the ICL HPC cx1
cluster) 
\end{tight_enumerate}

Default values can be selected by pressing {\textbf enter} without inputting
any data. An example of running the {\tt setup.sh} is shown below:

\begin{verbatim}
[jamesa@wssb-james TB-VSA_Pipeline]$ ./setup.sh 
Enter path to perl: [/usr/bin/perl] 
If this software is to be run on a PBSpro cluster requiring job submission to a particular queue, 
	enter the desired queue name (enter to skip): med-bio
Enter number of parallel threads to use: [4] 20

Installing Archive::Zip...
Installing Digest::MD5::File...
Installing LWP::UserAgent...
Installing XML::XPath...
Installing XML::XPath::Parser...

Updating scripts...
\end{verbatim}

Once the script has completed, appropriately configured versions of the scripts
will be available in the {\tt bin} directory of the software installation
directory. 

To make the scripts available directly on the path of your system (so the
scripts can be executed by typing their name without having to include the full
path to the script), it is necessary to modify the .bashrc file located in your
home directory with the inclusion of the following line:

\begin{verbatim}
export PATH=/path/to/installation/directory:${PATH}
\end{verbatim}

where {\tt /path/to/installation/directory} is replaced with the path to the
location where the software is installed.  It will be necessary to logout and
login again for this change to take effect. 

\section {Running the Pipeline}

The pipeline has been designed to be as easy as possible to run, and consists
of two major stages: Carrying out per-sample analysis and variant calling,
followed by the creation of the variable site alignment from the outputs of the
per-sample analysis. A separate instance of the per-sample analysis is executed
on multiple cluster nodes in parallel to minimise runtime, while the generation
of the variable site alignment is carried out by a single process on one node.

\subsection {Preparing to run the pipeline}

The following data are required to run the pipeline:

\begin{tight_enumerate}
\item A pair of gzip compressed fastq files for each isolate. 
\item A fasta-formatted genome sequence of the reference isolate
\item A BED file describing hyper-variable regions of the reference genome to exclude from the VSA.
\end{tight_enumerate}

\subsubsection {Input Sequences}

A new directory should be created for the input sequences for each pipeline
run. This should contain a separate directory for each sample, named with the
sample name, and contain the fastq files for the sample, named
[sample]\_1.fastq.gz and [sample]\_2.fastq.gz. No other files/directories
should be placed in the input directory since a list of samples to be analysed
will be determined by the software from a listing of the contents of this
directory.

\subsubsection {Reference genome}

The sequence of the isolate to be used as a reference should be available as a
fasta format file. An appropriately formatted file is available in the {\tt
data} directory of the pipeline installation for the H37Rv isolate. 

\subsection {Hyper-variable regions}

A list of genomic regions to exclude from the analysis (i.e. highly variable
regions which do not aid phylogenetic analysis ) can optionally be provided as
a BED format file. The BED format is a tab-delimited file format using one line
per feature, with 3-12 columns used to describe genomic locations. The pipeline
only requires the first three columns, which describe the reference identifier,
the start co-ordinate of the region and the end co-ordinate. Co-ordinates are
0-based, meaning the first base of the reference sequence is numbered base 0.
An example of part of valid BED file is shown in figure \ref{fig:1}. 

\begin{figure}
	\hrule
	\vskip\smallskipamount
	\begin{Verbatim}[baselinestretch=0.75]
AL123456.3       3736984         3738438
AL123456.3       1541994         1542980
AL123456.3       2430159         2431199
AL123456.3       2784657         2785697
AL123456.3       3120566         3121552 
\end{Verbatim}
	\hrule
	\vskip\smallskipamount
	\caption{Example of BED format file describing regions to be excluded from analysis}\label{fig:1}
\end{figure}

An example BED file for the H37Rv isolate is provided in the {\tt data}
directory of the pipeline installation.

\subsection {Running per-sample analysis}

\subsubsection {Cluster submission}

The pipeline is designed to be run on a cluster running the PBSPro job
scheduling software, with specific configurations defined for the Imperial
College HPC cx1 cluster. Modification to use different cluster
configurations/middleware will require the lines beginning {\tt \#PBS} in
the {\tt tb\_pipeline\_run} and {\tt build\_vsa\_run} scripts to be updated
according to the desired cluster configuration/queueing system. Additionally the
{\tt qsub} commands in {\tt submit\_tb\_run} and {\tt submit\_build\_vsa} will need
to be updated to suit the syntax of the required queue submission command. 

A single script ({\tt submit\_tb\_run}) needs to be run to carry out job
submission of the per-sample analysis for an entire dataset. Basic usage
information can be obtained by running the script without any arguments:

\begin{verbatim}
[jamesa@wssb-james colijn]$ submit_tb_run 
Usage: submit_tb_run -i /path/to/input/dir -o /path/to/output/dir -r /path/to/reference
\end{verbatim}

The script requires three arguments:

\begin{tight_itemize}
\item \textbf{-i}: Path to input directory. The fully-qualified path to
the directory described above containing a subdirectory of compressed fastq
files for each sample.
 \item \textbf{-o}: Path to output directory. The fully-qualified path to a
directory for storing intermediate run files and outputs in per-sample
subdirectories. This directory will be created automatically if it doesn't
exist. \textbf{N.B. The directory name will be used as a project name when reporting outputs}
\item \textbf{-r}: Path to reference. The fully qualified path to the
asta-formatted genome sequence of the reference isolate.
\end{tight_itemize}

The script will determine the number of samples being analysed by obtaining a
directory listing of the specified input directory, then submit an array job to
the cluster for the relevant number of tasks, with one task per sample. The
progress of these jobs can be monitored using the {\tt qstat -f} command i.e. 

\begin{verbatim}
-bash-4.2$ qstat -f
Job Id: 4830274[].cx1b
    Job_Name = analyse_tb_sample
    Job_Owner = jamesa@login-2-internal.cx1.hpc.ic.ac.uk
	[truncated for clarity]
    array_state_count = Queued:8 Running:0 Exiting:0 Expired:0 
    array_indices_submitted = 1-8
    array_indices_remaining = 1-8
    project = _pbs_project_default
\end{verbatim}

This example {\tt qstat -f} output shows an array of 8 tasks which have been
submitted, all of which are queued pending execution.  Each task will execute a
single instance of the {\tt tb\_pipeline\_run} script, which is a queue
execution wrapper around {\tt analyse\_tb\_sample}.

\subsubsection {Analysing a single sample}

Individual sample analysis can be running the {\tt analyse\_tb\_sample} command
directly e.g for running outside a cluster environment. It's usage is as follows:

\begin{verbatim}
analyse_tb_sample --input input_directory --output output_directory \
   --reference /path/to/reference.fasta [--sample sample_name]
\end{verbatim}

\begin{tight_itemize}
\item \textbf{--input}: Path to input directory. The fully-qualified path to
the directory described above containing a subdirectory of compressed fastq
files for each sample.
 \item \textbf{-output}: Path to output directory. The fully-qualified path to a
directory for storing intermediate run files and outputs in per-sample
subdirectories. This directory will be created automatically if it doesn't
exist. 
\item \textbf{-reference}: Path to reference. The fully qualified path to the
fasta-formatted genome sequence of the reference isolate.
\item \textbf{--sample}: Sample name to analyse (must match directory name
containing sample's reads in the input directory) 
\end{tight_itemize}

Note that the script will refuse to run if an output directory already exists
for the defined sample, consequently it will be necessary to remove existing
output directories or specific a different output directory should it be
necessary to rerun the analysis of any samples. 

\subsubsection{Identifying failed tasks}

There is a possibility that individual sample analysis jobs may fail, for
example if they exceed the job limits defined by the submission script i.e.
memory requirement, or runtime. Following the completion of all the scheduled
tasks, it is possible to identify any which have failed using the {\tt
find\_failed\_tasks} script.

\begin{verbatim}
find_failed_tasks --input input_directory --output output_directory [--clean]
\end{verbatim}

\begin{tight_itemize}
\item \textbf{--input}: Path to input directory specified during job submission
\item \textbf{--output}: Path to output directory specified during job submission
\item \textbf{--clean} (optional): Remove output directories of failed jobs
\end{tight_itemize}

When run, this script will report any samples which have failed the analysis
run. The standard output/standard error of any failed tasks should be inspected
to identify the cause of the failure. 

The script can also be run with the {\tt clean} flag which will remove the
output directory of any failed samples. Once the cause of the job failures has
been addressed, {\tt submit\_tb\_run} should be rerun as before. Any samples which
have existing output directories will be skipped, while those which are missing
from previously failed jobs will be rerun. 

This process should be repeated until all samples have successfully completed analysis.

\subsubsection{Identifying outliers}

Outliers can be automatically identified using the {\tt identify\_outliers}
script. Any samples identified as being overly divergent from the majority of
the data will have their sample ids written to an {\tt 'outliers.txt'} file in
the analysis directory. This will be read by subsequent pipeline stages and any
samples listed in this file excluded from further analysis.

The {\tt identify\_outliers} script requires two arguments:

\begin{verbatim}
identify_outliers -i /path/to/analysis/dir -r /path/to/reference/seq
\end{verbatim}

\begin{tight_itemize}
\item \textbf{-i}: Path to analysis directory (output directory specified during previous job submission)
\item \textbf{-r}: Path to fasta-format reference sequence
\end{tight_itemize}

\subsubsection{Generating a variable site alignment}

Once all the tasks have successfully completed, the {\tt build\_vsa} script can
be used to generate a variable site alignment from the samples. This can be run
interactively if resources allow, or submitted to a cluster using the {\tt
submit\_build\_vsa} script. Note that when run on the Imperial HPC systems, the
job should be submitted to cx1's queues.

\begin{Verbatim}
submit_build_vsa -d /path/to/analysis/dir -r /path/to/reference \
	-e /path/to/exclude.bed -o /path/to/outliers.txt
\end{Verbatim}

The following arguments should be passed to the {\tt submit\_build\_vsa} script:

\begin{tight_itemize}
\item \textbf{-d}: Path to analysis directory (output directory specified during job submission)
\item \textbf{-r}: Path to fasta formatted reference sequence
\item \textbf{-e}: Path to bed format file describing genomic regions to exclude from VSA
\item \textbf{-o}: Path to text file listing outliers, one sample id per line
\end{tight_itemize}

The resulting variable site alignment will be written to the output directory
as a fasta-formatted alignment, along with an index file indicating the genomic
locus represented by each position of the alignment.

\section{Pipeline Workflow}

The pipeline consists of a series of operations. Many of the tools and
algorithms used can be readily replaced with alternatives if desired, or
runtime parameters altered by editing the command lines defined in the {\tt
analyse\_tb\_sample} script. 

\subsection{QC}

An assessment of sequence quality is initially carried out using FastQC. This
carries out a separate quality assessment on each of read-pairs taking account
of a considerable number of metrics. A rating of PASS, WARN or FAIL is assigned
for each metric. These ratings are somewhat arbitrary distinctions but for the
most part hold true. It is also important to understand the basis of each
metric and what may cause abnormal results. For example kmer content and
overrepresented sequences may well result from adapter contaminations, which
will be handled automatically by the pipeline. 

Text format summaries are generated for each fastq file and can be found in the
per-sample output directory name samplename\_fastqc.summary.txt. Should further
investigation of the results be required, a {\tt fastqc} subdirectory is
present in each sample output directory which contains HTML format outputs for
each fastq file which can be viewed with a web browser. Documentation on
interpreting the various quality statistics is available from
\href{https://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3\%20Analysis\%20Modules}{'https://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3\%20Analysis\%20Modules/'}.

\subsection{Read trimming}

Reads are next trimmed using trim\_galore to remove residual adapter sequences
and bases of low quality. Bases with with quality scores below 20 are removed
from the 3' end of the reads, as well as sequences matching standard Illumina
adapters. Any reads which are shorter than 70bp following quality trimming are
discarded.

\subsection{Read alignment}

Reads are aligned to the reference genome using the BWA MEM algorithm, using
default parameters, and the outputs converted to bam format using sambamba.
Read group information is then added to the bam files to ensure compatibility
with downstream tools such as GATK which require bam files to have read groups
assigned. This process results in a sorted, indexed bam file which includes a
{\tt RG} header line describing the read group, and an associated RG tag on
each read.

\subsection{Marking duplicates}

Duplicate reads should occur at a very low frequency at the depths of
sequencing typically employed, and add no meaningful information. PCR
over-amplification can result in over-representation of particular reads which
can influence variant calling accuracy. These duplicate reads therefore need to
be identified and marked in the bam files to ensure they are not included in
downstream analysis.  Duplicates are identified using Picard's MarkDuplicates
tool, and the resulting bam file indexed.

\subsection{Local realignment round indels}

Each read-pair is aligned independently, resulting in the identification of the
optimal alignment for each individual read pair. When reads span indel loci,
the best alignment of each read-pair may differ between read-pairs resulting in
some 'left-justified' alignments, and some which are 'right-justified' (see
figure \ref{fig:2}). As well as not correctly representing the indel locus,
these alignments also introduce a number of false-positive SNV loci in the
region surrounding the indel. The process of local indel realignment identifies
potential indel loci and carries out a realignment of the reads spanning this
region, only carries out the alignment in the context of all the reads to
produce a best alignment of all the reads together. This not only results in
the indel being correctly represented in the alignments, but also removes the
spurious SNV calls.

Some variant callers carry out this process themselves, but to allow the
variant caller in the pipeline to be readily substituted a standalone
realignment task is carried out to allow the use of variant callers which do
not take indel loci into account in this manner.  This is carried out using the
GATK IndelRealigner tool. 

\begin{figure}
	\hrule
	\vskip\smallskipamount
	\begin{Verbatim}[baselinestretch=0.75]
ref:    ttataaaac----aattaagt     ttataaaac----aattaagt
sample: ttataaaacAAATaattaagt     ttataaaacAAATaattaagt
        ---------------------     ---------------------
read1:  ttataaaac    aaAtaa       ttataaaacAAATaa
read2:  ttataaaac    aaAtaaTt     ttataaaacAAATaatt     
read3:  ttataaaacAAATaattaagt     ttataaaacAAATaattaagt
read4:      CaaaT    aattaagt             cAAATaattaagt 
read5:        aaT    aattaagt               AATaattaagt
read6:          T    aattaagt                 Taattaagt
	\end{Verbatim}
	\hrule
	\vskip\smallskipamount
	\caption{Example of misalignment surround indel locus, both before (left) and after (right) local indel realignment. Variant bases are indicated in upper case }\label{fig:2}
\end{figure}

\subsection{SNV calling}

SNVs are identified using FreeBayes, a bayesian variant caller which allows
ploidy to be taken into account when identifying variants. The current
iteration of the pipeline has been developed purely with SNVs in mind, so
FreeBayes is run so as not to attempt identification of MNPs, INDELs or complex
variants (which are combinations of other variant types). Standard quality
filters are used, requiring a minimum mapping quality of 30 and minimum base
quality of 20. The minimum alternate fraction is also defined as 0.8, which
will only reports on variants identified in >80\% of the reads at a locus. This
helps reduce false-positives which is appropriate for sequence from a clonal
isolate but will result in missed variant calls in clinical samples containing
co-infecting isolates.

The parameters used for FreeBayes variant calling are as follows:

\begin{tight_itemize}
\item \textbf{-p 1}: Ploidy = 1 (haploid)
\item \textbf{-i}  : no INDELs
\item \textbf{-X}  : no MNPs
\item \textbf{-u}  : no complex
\item \textbf{-j}  : use mapping quality
\item \textbf{-0}  : standard quality filters - min mapping quality 30; min base quality 20; min supporting allele qsum 0; genotype variant threshold 0
\item \textbf{-F}  : min alternate fraction 0.8 
\end{tight_itemize}

\subsection{Identifying outliers}

Some samples may not have issues during sequencing, or simply be overly
divergent from the reference sequence to be sensible included in a variable
site alignment. These can be automatically identified using the {\tt
find\_outliers} script. This consolidates per-sample statistics from output
files in each samples output directory into a single file of samples statistics
for the whole project, then calls the 'outlier\_plots.R' script to identify
outliers and produce plots illustrating how these compare to the distribution
of the full set of samples.

Samples are identified as outliers if
\begin{tight_itemize}
\item have less than 20x genomic coverage
\item have a mapping rate below 90%
\item have an unusually high number of SNVs identified (>3x the interquartile range)
\end{tight_itemize}

The distribution of read-count/SNVs and sample/mapping-rate are plotted in PDF
files (named {\tt projectname\_snv\_dist.pdf} and
{\tt projectname\_mapping\_rate.pdf}), with identified outliers highlighted and
labelled with the sample ID. Outliers are also written to a {\tt
projectname\_outliers.txt} file in the analysis directory, which is read during
generation of the variable-site alignment and samples listed in it excluded
from the analysis. Sample IDs can be added to or removed from this file
manually as required to add or remove outliers from the dataset. 

\subsection{Generating variable-site alignment}

The variable-site alignment is essentially a fasta-format multiple sequence
alignment which excludes non-variant bases.  An array (@variants) is used to
track which loci are variant, and is initialised with one element for each base
of the reference, each element being set to '0'. The VCF output files from the
variant calling are then sequentially processed, and variant loci in each
sample parsed and stored in a hash (sample\_vars), keyed on the sample name and
containing an array reference to a list of variant bases identified for the
locus. For each variant identified at a locus, the value of the variants array
representing the locus is incremented. 

Once all VCF files have been parsed, the variants array is processed
incrementally. When a variant locus is identified, each of the samples in the
sample\_vars hash is examined and if a variant locus has been identified in
that sample, then the variant base is included in the alignment for that
sample, otherwise the corresponding reference allele is used. 

Following processing of all variant loci, the alignment is written, including
the loci from the reference sequence as the first line of the alignment. An
index is also created mapping the base number of the alignment to the
corresponding location on the reference genome.

\textbf{N.B.} At present only a single variant base is included for each sample
in the output alignment. This needs to be updated to produce the relevant IUPAC
ambiguity code in the event that multiple variant bases are identified i.e in a
non-clonal sample. 

\section {Additional scripts}

The {\tt bin} directory of the software installation includes a number of
additional scripts which may be helpful in using the pipeline. 

\subsubsection {download\_ena\_project} 

The {\tt download\_ena\_project} script is used to download all the sequence
reads for a given ENA project, which typically consists of hundreds of files
which otherwise require downloading independently. It is necessary to provide
the script with the ID of the ENA project (also referred to as an ENA Study),
and the path to the directory where the data should be downloaded. The
downloaded data will be arranged with the sequence read files for each sample
in a separate subdirectory, named with the sample identifier, as required by
the sample analysis process. An example of downloading a project is show below:

\begin{verbatim}
download_ena_project --acc ERP004677 --output reads/PRJEB5280
\end{verbatim}

\end{document}
